<!DOCTYPE html>
<html lang="en" dir=>

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Download FILENAME.ipynb notebook LevelDB parameter tuning using MLOS What is Level DB LevelDB is a key value store built using Log Structured Merge Trees (LSMs) Wiki. LevelDB supports read, write, delete and range query (sorted iteration) operations.
Typical to any database system, LevelDB also comes with a bunch of parameters which can be tuned according to the workload to get the best performance. Before going to the parameters, we&rsquo;ll briefly describe the working of LevelDB.">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="" />
<meta property="og:description" content="Download FILENAME.ipynb notebook LevelDB parameter tuning using MLOS What is Level DB LevelDB is a key value store built using Log Structured Merge Trees (LSMs) Wiki. LevelDB supports read, write, delete and range query (sorted iteration) operations.
Typical to any database system, LevelDB also comes with a bunch of parameters which can be tuned according to the workload to get the best performance. Before going to the parameters, we&rsquo;ll briefly describe the working of LevelDB." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://microsoft.github.io/MLOS/notebooks/LevelDbTuning/" />

<title>Level Db Tuning | MLOS</title>
<link rel="manifest" href="/MLOS/manifest.json">
<link rel="icon" href="/MLOS/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/MLOS/book.min.6cd8553a6854f4812343f0f0c8baca31271e686434f381fbe3c7226f66639176.css" integrity="sha256-bNhVOmhU9IEjQ/DwyLrKMSceaGQ084H748cib2ZjkXY=">
<script defer src="/MLOS/en.search.min.a358000ca6b684654d0631622f0004e31ca8e484da0fe6cd977d77432a0eb193.js" integrity="sha256-o1gADKa2hGVNBjFiLwAE4xyo5ITaD&#43;bNl313QyoOsZM="></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body dir=>
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/MLOS"><span>MLOS</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>











  <ul>
<li>
<p><a href="/MLOS/documentation/">Documentation</a></p>
<ul>
<li><a href="/MLOS/documentation/01-Prerequisites/">Prerequisites</a></li>
<li><a href="/MLOS/documentation/02-Build/">Build</a></li>
<li><a href="/MLOS/documentation/04-Test/">Test</a></li>
<li><a href="/MLOS/documentation/CodingStandard/">Coding Standard</a></li>
<li><a href="/MLOS/documentation/MlosArchitecture/">Architecture</a></li>
<li><a href="/MLOS/documentation/RepoOrganization/">Repo Organization</a></li>
</ul>
</li>
<li>
<p><a href="/MLOS/notebooks/">Notebooks</a></p>
<ul>
<li><a href="/MLOS/notebooks/BayesianOptimization/">Bayesian Optimization</a></li>
<li><a href="/MLOS/notebooks/SmartCacheOptimization/">Smart Cache Optimization</a></li>
<li><a href="/MLOS/notebooks/SmartCacheCPP/">Smart Cache Optimization in C++</a></li>
<li><a href="/MLOS/notebooks/LevelDbTuning/"class=active>LevelDB External Tuning Example</a></li>
</ul>
</li>
<li>
<p>API Documentation</p>
<ul>
<li><a href="/MLOS/python_api/">Python</a></li>
</ul>
</li>
<li>
<p><a href="/MLOS/source/Examples/">E2E Examples</a></p>
<ul>
<li><a href="/MLOS/source/Examples/SmartCache/">Smart Cache</a></li>
</ul>
</li>
<li>
<p><a href="/MLOS/CODE_OF_CONDUCT/">Code of Conduct</a></p>
</li>
<li>
<p><a href="/MLOS/CONTRIBUTING/">Contributing</a></p>
</li>
<li>
<p><a href="https://github.com/Microsoft/MLOS">MLOS Github Repository</a></p>
</li>
</ul>










</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/MLOS/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Level Db Tuning</strong>

  <label for="toc-control">
    
    <img src="/MLOS/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  <nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-level-db">What is Level DB</a></li>
    <li><a href="#leveldb-working">LevelDB working</a></li>
    <li><a href="#leveldb-paramter-tuning-using-mlos">LevelDB paramter tuning using MLOS</a></li>
    <li><a href="#leveldb-installation-instruction-on-ubuntu-1804">LevelDB installation: Instruction on Ubuntu 18.04</a>
      <ul>
        <li><a href="#verification">Verification</a></li>
      </ul>
    </li>
    <li><a href="#going-further">Going further</a>
      <ul>
        <li><a href="#reference">Reference</a></li>
      </ul>
    </li>
  </ul>
</nav>


  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="download-filenameipynb-notebookhttpsrawgithubusercontentcommicrosoftmlosmainexternalleveldbleveldbtuningipynb"><a href="https://raw.githubusercontent.com/microsoft/MLOS/main/external/leveldb/LevelDbTuning.ipynb">Download FILENAME.ipynb notebook</a></h1>
<h1 id="leveldb-parameter-tuning-using-mlos">LevelDB parameter tuning using MLOS</h1>
<h2 id="what-is-level-db">What is Level DB</h2>
<p>LevelDB is a key value store built using Log Structured Merge Trees (LSMs) <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree">Wiki</a>. LevelDB supports read, write, delete and range query (sorted iteration) operations.</p>
<p>Typical to any database system, LevelDB also comes with a bunch of parameters which can be tuned according to the workload to get the best performance. Before going to the parameters, we&rsquo;ll briefly describe the working of LevelDB. The source code, the architecture and a simple example of how to use LevelDB can be found <a href="https://github.com/google/leveldb">here</a>.</p>
<h2 id="leveldb-working">LevelDB working</h2>
<p><img src="/MLOS/notebooks/images/leveldb-architecture.png" alt="LevelDB Architecture Diagram" /></p>
<p><img src="/MLOS/notebooks/images/memtablesstable.png" alt="MemTable SSTable Diagrams" /></p>
<p>LevelDB uses 7 levels to store the data, the amoung of data that can be stored in each of the levels after level 0 is $10^{level}$, so level 1 can store around 10 MB of data, level 2 around 100 MB and so on.</p>
<p>As shown the diagram above, the main components of LevelDB are the <em>MemTable</em>,the <em>SSTable</em> files and the <em>log</em> file. LeveDB is primarily optimized for writes.</p>
<p><em>MemTable</em> is an in memory data structure to which incoming writes are added after they are appended to the log file. MemTables are typically implemented using skip lists or B+ trees. The parameter <code>write_buffer_size</code> (paramter input at DB startup) can be used to control the size of the MemTable and the log file.</p>
<p>Once the MemTable reaches the <code>write_buffer_size</code> (Default 4MB), a new MemTable is created and the original MemTable is made immutable. This immutable MemTable is converted to a new SSTable in the background to be added to the Level 0 of the LSM tree.</p>
<p><em>SSTable</em>: It is a file in which the key value pairs are stored sorted by keys. The size of SSTable is controlled by the parameter called max_file_size (Default 2MB).</p>
<p>Once the number of SSTable at Level 0 reaches a certain threshold controlled by the paramter <code>kL0_CompactionTrigger</code> (Default 4), these files are merged with higher level overlapping files. If no files are present in the higher level, the files are combined using merge sort techniques and added to higher level. A new file is created for every 2 MB of data by default.</p>
<p>For higher levels from 1 to the maximum number of levels, compaction process (merging process) is triggered when the level gets filled.</p>
<p>A detailed explanation of the working of LeveDB is presented <a href="https://github.com/google/leveldb/blob/master/doc/impl.md">here</a>.</p>
<h2 id="leveldb-paramter-tuning-using-mlos">LevelDB paramter tuning using MLOS</h2>
<p>In this lab we will be tuning some of the important start up time paramters of LevelDB and observe how it affects the performance. The parameters that we will be tuning are <code>write_buffer_size</code> and <code>max_file_size</code> to try to optimize the throughput and latency of LevelDB for Sequential and random workloads.</p>
<h2 id="leveldb-installation-instruction-on-ubuntu-1804">LevelDB installation: Instruction on Ubuntu 18.04</h2>
<p>Follow the commands below to get, compile and install LevelDB</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">sudo apt update
sudo apt-get install cmake
git clone --recurse-submodules https://github.com/google/leveldb.git
cd leveldb
mkdir -p build <span style="color:#f92672">&amp;&amp;</span> cd build
cmake -DCMAKE_BUILD_TYPE<span style="color:#f92672">=</span>Release .. <span style="color:#f92672">&amp;&amp;</span> cmake --build .
</code></pre></div><p>Now, from the <code>~/leveldb/build</code> directory, you should be able to execute <code>./db_bench</code>, the microbenchmark which can be used to measure the performance of LevelDB for different workloads.</p>
<p>Please take a look at the <code>db_bench.cc</code> file in the <code>~/leveldb/benchmarks</code> directory and get an idea about the input parameters and workloads that are possible.</p>
<p>An example command to run a workload that does random writes of 1M values with value size 100 B is:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-sh" data-lang="sh">./db_bench --benchmarks<span style="color:#f92672">=</span>fillrandom --val_size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span> --num<span style="color:#f92672">=</span><span style="color:#ae81ff">1000000</span>
</code></pre></div><p>The output of the command will look like (numbers migth be different):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-txt" data-lang="txt">LevelDB:    version 1.22
Date:       Thu Oct  8 13:56:00 2020
CPU:        40 * Intel(R) Xeon(R) CPU E5-2660 v3 @ 2.60GHz
CPUCache:   25600 KB
Keys:       16 bytes each
Values:     100 bytes each (50 bytes after compression)
Entries:    1000000
RawSize:    110.6 MB (estimated)
FileSize:   62.9 MB (estimated)
WARNING: Snappy compression is not enabled
------------------------------------------------
Opening the DB now
In the collect stats thread
Total data written = 421.9 MB   
fillrandom :      31.731 micros/op;    3.5 MB/s
</code></pre></div><p>In the subsequent cells, we will using Bayesian optimization in MLOS to tune the startup time parameters to obtain the parameters that result in best throughput and latency.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> subprocess
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">from</span> scipy.stats <span style="color:#f92672">import</span> t
<span style="color:#f92672">from</span> mlos.Optimizers.OptimizationProblem <span style="color:#f92672">import</span> OptimizationProblem, Objective
<span style="color:#f92672">from</span> mlos.Optimizers.BayesianOptimizer <span style="color:#f92672">import</span> BayesianOptimizer
<span style="color:#f92672">from</span> mlos.Spaces <span style="color:#f92672">import</span> SimpleHypergrid, ContinuousDimension, DiscreteDimension

<span style="color:#f92672">from</span> mlos.Optimizers.BayesianOptimizerConfigStore <span style="color:#f92672">import</span> bayesian_optimizer_config_store
<span style="color:#f92672">from</span> mlos.Optimizers.BayesianOptimizerFactory <span style="color:#f92672">import</span> BayesianOptimizerFactory
<span style="color:#f92672">from</span> mlos.Spaces <span style="color:#f92672">import</span> Point

<span style="color:#75715e"># configure the optimizer, start from the default configuration</span>
optimizer_config <span style="color:#f92672">=</span> bayesian_optimizer_config_store<span style="color:#f92672">.</span>default
<span style="color:#75715e"># set the fraction of randomly sampled configuration to 10% of suggestions</span>
optimizer_config<span style="color:#f92672">.</span>experiment_designer_config_fraction_random_suggestions <span style="color:#f92672">=</span> <span style="color:#f92672">.</span><span style="color:#ae81ff">1</span>
<span style="color:#75715e"># configure the random forest surrogate model</span>
random_forest_config <span style="color:#f92672">=</span> optimizer_config<span style="color:#f92672">.</span>homogeneous_random_forest_regression_model_config
<span style="color:#75715e"># refit the model after each observation</span>
random_forest_config<span style="color:#f92672">.</span>decision_tree_regression_model_config<span style="color:#f92672">.</span>n_new_samples_before_refit <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
<span style="color:#75715e"># Use the best split in trees (not random as in extremely randomized trees)</span>
random_forest_config<span style="color:#f92672">.</span>decision_tree_regression_model_config<span style="color:#f92672">.</span>splitter <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;best&#39;</span>
<span style="color:#75715e"># right now we&#39;re sampling without replacement so we need to subsample</span>
<span style="color:#75715e"># to make the trees different when using the &#39;best&#39; splitter</span>
random_forest_config<span style="color:#f92672">.</span>samples_fraction_per_estimator <span style="color:#f92672">=</span> <span style="color:#f92672">.</span><span style="color:#ae81ff">9</span>
<span style="color:#75715e"># Use 10 trees in the random forest (usually more are better, 10 makes it run pretty quickly)</span>
random_forest_config<span style="color:#f92672">.</span>n_estimators <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>
<span style="color:#75715e"># Set multiplier for the confidence bound</span>
optimizer_config<span style="color:#f92672">.</span>experiment_designer_config<span style="color:#f92672">.</span>confidence_bound_utility_function_config<span style="color:#f92672">.</span>alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.1</span>
<span style="color:#75715e"># optimizer = optimizer_factory.create_local_optimizer(</span>
<span style="color:#75715e">#     optimization_problem=optimization_problem,</span>
<span style="color:#75715e">#     optimizer_config=optimizer_config</span>
<span style="color:#75715e"># )</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># You might have to change the min and max value based on the start up time parameter that you want explore</span>
parameter_search_space <span style="color:#f92672">=</span> SimpleHypergrid(
        name<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;parameter_config&#39;</span>,
        dimensions<span style="color:#f92672">=</span>[
            DiscreteDimension(<span style="color:#e6db74">&#39;parameter&#39;</span>, min<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span><span style="color:#f92672">*</span><span style="color:#ae81ff">1024</span><span style="color:#f92672">*</span><span style="color:#ae81ff">1024</span>, max<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span><span style="color:#f92672">*</span><span style="color:#ae81ff">1024</span><span style="color:#f92672">*</span><span style="color:#ae81ff">1024</span>)
        ]
    )
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Optimization Problem</span>
<span style="color:#75715e"># You might have to change the min and max value based on the objective that you are using </span>
optimization_problem <span style="color:#f92672">=</span> OptimizationProblem(
    parameter_space<span style="color:#f92672">=</span>parameter_search_space,
    objective_space<span style="color:#f92672">=</span>SimpleHypergrid(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;objectives&#34;</span>, dimensions<span style="color:#f92672">=</span>[ContinuousDimension(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;objective&#34;</span>, min<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, max<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)]),
    objectives<span style="color:#f92672">=</span>[Objective(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;objective&#34;</span>, minimize<span style="color:#f92672">=</span>False)]
)

optimizer_factory <span style="color:#f92672">=</span> BayesianOptimizerFactory()
optimizer <span style="color:#f92672">=</span> optimizer_factory<span style="color:#f92672">.</span>create_local_optimizer(
    optimization_problem<span style="color:#f92672">=</span>optimization_problem,
    optimizer_config<span style="color:#f92672">=</span>optimizer_config
)

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">initialize_optimizer</span>():
    optimizer_factory <span style="color:#f92672">=</span> BayesianOptimizerFactory()
    optimizer <span style="color:#f92672">=</span> optimizer_factory<span style="color:#f92672">.</span>create_local_optimizer(
    optimization_problem<span style="color:#f92672">=</span>optimization_problem,
    optimizer_config<span style="color:#f92672">=</span>optimizer_config)
</code></pre></div><pre><code>ity_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:26 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:26 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:26 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:26 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:26 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:26 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:26 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Remember to initialize the optimizer before each time you run optimization or call run_optimizer() function</span>
initialize_optimizer()
</code></pre></div><pre><code>ity_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:27 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:27 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:27 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:27 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:27 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:27 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
10/08/2020 21:58:27 -   BayesianOptimizerFactory -    INFO - [BayesianOptimizerFactory.py:  40 -    create_local_optimizer() ] Creating a bayesian optimizer with config: {
  &quot;surrogate_model_implementation&quot;: &quot;HomogeneousRandomForestRegressionModel&quot;,
  &quot;experiment_designer_implementation&quot;: &quot;ExperimentDesigner&quot;,
  &quot;min_samples_required_for_guided_design_of_experiments&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.n_estimators&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator&quot;: 0.9,
  &quot;homogeneous_random_forest_regression_model_config.regressor_implementation&quot;: &quot;DecisionTreeRegressionModel&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion&quot;: &quot;mse&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter&quot;: &quot;best&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split&quot;: 2,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf&quot;: 3,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features&quot;: &quot;auto&quot;,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha&quot;: 0,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit&quot;: 10,
  &quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit&quot;: 1,
  &quot;homogeneous_random_forest_regression_model_config.bootstrap&quot;: 1,
  &quot;experiment_designer_config.utility_function_implementation&quot;: &quot;ConfidenceBoundUtilityFunction&quot;,
  &quot;experiment_designer_config.numeric_optimizer_implementation&quot;: &quot;RandomSearchOptimizer&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name&quot;: &quot;upper_confidence_bound_on_improvement&quot;,
  &quot;experiment_designer_config.confidence_bound_utility_function_config.alpha&quot;: 0.1,
  &quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration&quot;: 1000,
  &quot;experiment_designer_config.fraction_random_suggestions&quot;: 0.5,
  &quot;experiment_designer_config_fraction_random_suggestions&quot;: 0.1
}.
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Please change the leveldb_path to the build directory of your leveldb installation</span>
leveldb_path <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;$HOME/leveldb/build/&#34;</span>
<span style="color:#75715e"># You can change the command to run a different kind of workload (take a look at db_bench.cc to see the possible workloads)</span>
command <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;db_bench&#34;</span>

<span style="color:#75715e"># You might have to change the run workload function to explore a combination of parameters simultaneously</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_workload</span>(workload, input_parameter, parameter_value):
    <span style="color:#75715e"># The line below executes the db_bench command with approprite parameters, you can change this </span>
    <span style="color:#75715e"># if you want to specify other input parameters</span>
    result <span style="color:#f92672">=</span> subprocess<span style="color:#f92672">.</span>check_output(leveldb_path <span style="color:#f92672">+</span> command <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34; --benchmarks=&#34;</span> <span style="color:#f92672">+</span> workload <span style="color:#f92672">+</span>  <span style="color:#e6db74">&#34; --&#34;</span> <span style="color:#f92672">+</span> str(input_parameter) <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34;=&#34;</span> <span style="color:#f92672">+</span> str(parameter_value), shell<span style="color:#f92672">=</span>True)
    stats <span style="color:#f92672">=</span> (str(result)<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;:&#34;</span>)[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>])<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34;;&#34;</span>)
    <span style="color:#75715e"># The line below is used to parse the output that is returned by db_bench</span>
    latency, throughput <span style="color:#f92672">=</span> float(stats[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34; &#34;</span>)[<span style="color:#ae81ff">0</span>]), float(stats[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>strip()<span style="color:#f92672">.</span>split(<span style="color:#e6db74">&#34; &#34;</span>)[<span style="color:#ae81ff">0</span>])
    <span style="color:#66d9ef">return</span> latency, throughput

<span style="color:#75715e">#optimizer = bayesian_optimizer_factory.create_remote_optimizer(optimization_problem=optimization_problem)</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">run_optimizer</span>():
    <span style="color:#75715e"># Parameter 1: write_buffer_size: min_value = 1 MB, max_value = 128 MB</span>
    <span style="color:#75715e"># Parameter 2: max_file_size: min_value = 1 MB, max_value = 128 MB</span>
    <span style="color:#75715e"># Optimization parameters: latency and throughput, both returned by run_workload</span>
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">10</span>):
        new_config_values <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>suggest()
        new_parameter_value <span style="color:#f92672">=</span> new_config_values[<span style="color:#e6db74">&#34;parameter&#34;</span>]
        latency, throughput <span style="color:#f92672">=</span> run_workload(<span style="color:#e6db74">&#34;fillrandom&#34;</span>, <span style="color:#e6db74">&#34;max_file_size&#34;</span>, new_parameter_value)
        <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Parameter value: {}, Objective value: {}&#34;</span><span style="color:#f92672">.</span>format(str(new_parameter_value),  str(throughput) <span style="color:#f92672">+</span> <span style="color:#e6db74">&#34; MB/s&#34;</span>))
        <span style="color:#66d9ef">if</span> i <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>:
            optimum_parameter, optimum_value <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>optimum() 
            <span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#34;Optimal parameter: {}, Optimal value: {}&#34;</span><span style="color:#f92672">.</span>format(optimum_parameter[<span style="color:#e6db74">&#34;parameter&#34;</span>], optimum_value[<span style="color:#e6db74">&#34;objective&#34;</span>]))
        objectives_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({<span style="color:#e6db74">&#39;objective&#39;</span>: [throughput]})
        features_df <span style="color:#f92672">=</span> new_config_values<span style="color:#f92672">.</span>to_dataframe()
        optimizer<span style="color:#f92672">.</span>register(features_df, objectives_df)

<span style="color:#75715e"># Remember to call initialize_optimizer function before the run_optimizer</span>
<span style="color:#75715e"># To avoid the optimizer remembering the optimal values from previous run</span>
run_optimizer()
</code></pre></div><pre><code>Parameter value: 14231436, Objective value: 21.8 MB/s
Parameter value: 7465366, Objective value: 21.1 MB/s
Optimal parameter: 14231436, Optimal value: 21.8
Parameter value: 61708892, Objective value: 21.9 MB/s
Optimal parameter: 14231436, Optimal value: 21.8
Parameter value: 13801008, Objective value: 21.5 MB/s
Optimal parameter: 61708892, Optimal value: 21.9
Parameter value: 37672330, Objective value: 21.3 MB/s
Optimal parameter: 61708892, Optimal value: 21.9
Parameter value: 56044662, Objective value: 21.7 MB/s
Optimal parameter: 61708892, Optimal value: 21.9
Parameter value: 4266076, Objective value: 20.8 MB/s
Optimal parameter: 61708892, Optimal value: 21.9
Parameter value: 25616874, Objective value: 22.4 MB/s
Optimal parameter: 61708892, Optimal value: 21.9
Parameter value: 55095476, Objective value: 20.5 MB/s
Optimal parameter: 25616874, Optimal value: 22.4
Parameter value: 2542654, Objective value: 22.0 MB/s
Optimal parameter: 25616874, Optimal value: 22.4
</code></pre>
<h3 id="verification">Verification</h3>
<p>Manually run the benchmark for various values of the parameter that you are testing, plot the graphs and verify if the optimal returned by the optimizer matches with the one manually obtained.
For example, if the <code>input_parameter</code> is <code>write_buffer_size</code>, you can start from 2 MB (2097152) and go up to 64 MB (67108864), by trying values like, 2MB, 4MB, 8MB, 16MB, 32MB, 64MB and verify the point of deflection i.e the point where throughput starts to decrease after increasing or latency starts to increase after decreasing and verify if it matches with what is returned by the optimizer.</p>
<h2 id="going-further">Going further</h2>
<ol>
<li>Choose 2 parameters from <code>leveldb/include/leveldb/options.h</code> file (this can include <code>write_buffer_size</code> and <code>max_file_size</code>), and try to tune them manually and using the optimizer and compare the results.</li>
</ol>
<h3 id="reference">Reference</h3>
<ul>
<li><a href="https://wiesen.github.io/post/leveldb-storage-memtable/">https://wiesen.github.io/post/leveldb-storage-memtable/</a></li>
<li><a href="https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/">https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/</a></li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">
</code></pre></div><h1 id="download-filenameipynb-notebookhttpsrawgithubusercontentcommicrosoftmlosmainexternalleveldbleveldbtuningipynb-1"><a href="https://raw.githubusercontent.com/microsoft/MLOS/main/external/leveldb/LevelDbTuning.ipynb">Download FILENAME.ipynb notebook</a></h1>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>

 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      
  <nav id="TableOfContents">
  <ul>
    <li><a href="#what-is-level-db">What is Level DB</a></li>
    <li><a href="#leveldb-working">LevelDB working</a></li>
    <li><a href="#leveldb-paramter-tuning-using-mlos">LevelDB paramter tuning using MLOS</a></li>
    <li><a href="#leveldb-installation-instruction-on-ubuntu-1804">LevelDB installation: Instruction on Ubuntu 18.04</a>
      <ul>
        <li><a href="#verification">Verification</a></li>
      </ul>
    </li>
    <li><a href="#going-further">Going further</a>
      <ul>
        <li><a href="#reference">Reference</a></li>
      </ul>
    </li>
  </ul>
</nav>

 
    </aside>
    
  </main>

  
</body>

</html>












