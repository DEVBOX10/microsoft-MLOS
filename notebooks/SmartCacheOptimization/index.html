<!DOCTYPE html>
<html lang="en" dir=>

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Download SmartCacheOptimization.ipynb notebook Optimizing Smart Cache with Bayesian Optimization The goal of this notebook is to optimize SmartCache using Bayesian Optimization approach.
We&rsquo;re using a sequential model-based optimization approach, that consists of the following loop:
 Get suggested config from optimizer, Apply suggested config to SmartCache, Execute a fixed workload, Collect the metrics from SmartCache, Register an observation with the optimizer.  # import the required classes and tools import grpc import pandas as pd import logging from mlos.">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="" />
<meta property="og:description" content="Download SmartCacheOptimization.ipynb notebook Optimizing Smart Cache with Bayesian Optimization The goal of this notebook is to optimize SmartCache using Bayesian Optimization approach.
We&rsquo;re using a sequential model-based optimization approach, that consists of the following loop:
 Get suggested config from optimizer, Apply suggested config to SmartCache, Execute a fixed workload, Collect the metrics from SmartCache, Register an observation with the optimizer.  # import the required classes and tools import grpc import pandas as pd import logging from mlos." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://microsoft.github.io/MLOS/notebooks/SmartCacheOptimization/" />

<title>Smart Cache Optimization | MLOS</title>
<link rel="manifest" href="/MLOS/manifest.json">
<link rel="icon" href="/MLOS/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/MLOS/book.min.6cd8553a6854f4812343f0f0c8baca31271e686434f381fbe3c7226f66639176.css" integrity="sha256-bNhVOmhU9IEjQ/DwyLrKMSceaGQ084H748cib2ZjkXY=">
<script defer src="/MLOS/en.search.min.0f92261397673edbf259d14496337c0a6ace44d6d9e9da2169a29706b54f2c97.js" integrity="sha256-D5ImE5dnPtvyWdFEljN8CmrORNbZ6dohaaKXBrVPLJc="></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
</head>

<body dir=>
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/MLOS"><span>MLOS</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="Search" aria-label="Search" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>











  <ul>
<li>
<p><a href="/MLOS/documentation/">Documentation</a></p>
<ul>
<li><a href="/MLOS/documentation/01-Prerequisites/">Prerequisites</a></li>
<li><a href="/MLOS/documentation/02-Build/">Build</a></li>
<li><a href="/MLOS/documentation/04-Test/">Test</a></li>
<li><a href="/MLOS/documentation/CodingStandard/">Coding Standard</a></li>
<li><a href="/MLOS/documentation/MlosArchitecture/">Architecture</a></li>
<li><a href="/MLOS/documentation/RepoOrganization/">Repo Organization</a></li>
</ul>
</li>
<li>
<p><a href="/MLOS/notebooks/">Notebooks</a></p>
<ul>
<li><a href="/MLOS/notebooks/BayesianOptimization/">Bayesian Optimization</a></li>
<li><a href="/MLOS/notebooks/SmartCacheOptimization/"class=active>Smart Cache Optimization</a></li>
</ul>
</li>
<li>
<p>API Documentation</p>
<ul>
<li><a href="/MLOS/python_api/">Python</a></li>
</ul>
</li>
<li>
<p><a href="/MLOS/source/Examples/">E2E Examples</a></p>
<ul>
<li><a href="/MLOS/source/Examples/SmartCache/">Smart Cache</a></li>
</ul>
</li>
<li>
<p><a href="/MLOS/CODE_OF_CONDUCT/">Code of Conduct</a></p>
</li>
<li>
<p><a href="/MLOS/CONTRIBUTING/">Contributing</a></p>
</li>
<li>
<p><a href="https://github.com/Microsoft/MLOS">MLOS Github Repository</a></p>
</li>
</ul>










</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/MLOS/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>Smart Cache Optimization</strong>

  <label for="toc-control">
    
    <img src="/MLOS/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  <nav id="TableOfContents"></nav>


  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="download-smartcacheoptimizationipynb-notebookhttpsrawgithubusercontentcommicrosoftmlosmainsourcemlosnotebookssmartcacheoptimizationipynb"><a href="https://raw.githubusercontent.com/microsoft/MLOS/main/source/Mlos.Notebooks/SmartCacheOptimization.ipynb">Download SmartCacheOptimization.ipynb notebook</a></h1>
<h1 id="optimizing-smart-cache-with-bayesian-optimization">Optimizing Smart Cache with Bayesian Optimization</h1>
<p>The goal of this notebook is to optimize SmartCache using Bayesian Optimization approach.</p>
<p>We&rsquo;re using a sequential model-based optimization approach, that consists of the following loop:</p>
<ol>
<li>Get suggested config from optimizer,</li>
<li>Apply suggested config to <code>SmartCache</code>,</li>
<li>Execute a fixed workload,</li>
<li>Collect the metrics from <code>SmartCache</code>,</li>
<li>Register an observation with the optimizer.</li>
</ol>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># import the required classes and tools</span>
<span style="color:#f92672">import</span> grpc
<span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> logging

<span style="color:#f92672">from</span> mlos.Logger <span style="color:#f92672">import</span> create_logger

<span style="color:#f92672">from</span> mlos.Examples.SmartCache <span style="color:#f92672">import</span> HitRateMonitor, SmartCache, SmartCacheWorkloadGenerator, SmartCacheWorkloadLauncher
<span style="color:#f92672">from</span> mlos.Mlos.SDK <span style="color:#f92672">import</span> MlosExperiment
<span style="color:#f92672">from</span> mlos.Optimizers.BayesianOptimizerFactory <span style="color:#f92672">import</span> BayesianOptimizerFactory
<span style="color:#f92672">from</span> mlos.Optimizers.OptimizationProblem <span style="color:#f92672">import</span> OptimizationProblem, Objective
<span style="color:#f92672">from</span> mlos.Spaces <span style="color:#f92672">import</span> Point, SimpleHypergrid, ContinuousDimension

<span style="color:#75715e"># The optimizer will be in a remote process via grpc, we pick the port here:</span>
grpc_port <span style="color:#f92672">=</span> <span style="color:#ae81ff">50051</span>
</code></pre></div><p>Launch the optimizer service in a different process:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> subprocess
optimizer_microservice <span style="color:#f92672">=</span> subprocess<span style="color:#f92672">.</span>Popen(f<span style="color:#e6db74">&#34;start_optimizer_microservice launch --port {grpc_port}&#34;</span>, shell<span style="color:#f92672">=</span>True)
</code></pre></div><p>Now the optimizer service that runs the surrogate model and suggests new points is started in the background.
Next, we instantiate an object that connects to it over grpc using the <code>BayesianOptimizerFactory</code>.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">logger <span style="color:#f92672">=</span> create_logger(<span style="color:#e6db74">&#39;Optimizing Smart Cache&#39;</span>, logging_level<span style="color:#f92672">=</span>logging<span style="color:#f92672">.</span>WARN)
optimizer_service_grpc_channel <span style="color:#f92672">=</span> grpc<span style="color:#f92672">.</span>insecure_channel(f<span style="color:#e6db74">&#39;localhost:{grpc_port}&#39;</span>)
bayesian_optimizer_factory <span style="color:#f92672">=</span> BayesianOptimizerFactory(grpc_channel<span style="color:#f92672">=</span>optimizer_service_grpc_channel, logger<span style="color:#f92672">=</span>logger)
</code></pre></div><h1 id="the-optimization-problem">The optimization problem</h1>
<p>Then we can instantiate our optimization problem. We want to optimize the configuration of the <code>SmartCache</code> component that contains two implementations: an LRU (least recently used) cache and an MRU cache (most recently used).
The <code>SmartCache</code> component has two parameters that we can adjust, the type of cache and the cache size. We are using some synthetic workloads for the cache and try to find what the optimum configuration for each workload is.</p>
<p>Here, we measure &lsquo;optimum&rsquo; by the number of cache hits. Another option would be to measure runtime; however, this is a toy example with a trivial workload and there is likely substantial runtime difference.
The parameter search space is declared in <code>SmartCache.parameter_search_space</code>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">SmartCache<span style="color:#f92672">.</span>parameter_search_space
</code></pre></div><pre><code>  Name: smart_cache_config
  Dimensions:
    implementation: {LRU, MRU}

  IF implementation IN {LRU} THEN (
    Name: lru_cache_config
    Dimensions:
      cache_size: {1, 2, ... , 4096}
  )

  IF implementation IN {MRU} THEN (
    Name: mru_cache_config
    Dimensions:
      cache_size: {1, 2, ... , 4096}
  )
</code></pre>
<p>The optimization problem is constructed using this parameter space as the input to optimize, and defines a single continuous objective, &lsquo;hit_rate&rsquo; between 0 and 1.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Optimization Problem</span>
<span style="color:#75715e">#</span>
optimization_problem <span style="color:#f92672">=</span> OptimizationProblem(
    parameter_space<span style="color:#f92672">=</span>SmartCache<span style="color:#f92672">.</span>parameter_search_space,
    objective_space<span style="color:#f92672">=</span>SimpleHypergrid(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;objectives&#34;</span>, dimensions<span style="color:#f92672">=</span>[ContinuousDimension(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;hit_rate&#34;</span>, min<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, max<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)]),
    objectives<span style="color:#f92672">=</span>[Objective(name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;hit_rate&#34;</span>, minimize<span style="color:#f92672">=</span>False)]
)
<span style="color:#75715e"># create an optimizer proxy that connects to the remote optimizer via grpc:</span>
optimizer <span style="color:#f92672">=</span> bayesian_optimizer_factory<span style="color:#f92672">.</span>create_remote_optimizer(optimization_problem<span style="color:#f92672">=</span>optimization_problem)
</code></pre></div><h1 id="defining-workloads">Defining workloads</h1>
<p>Now we can instantiate our workloads and stand up the MLOS infrastructure, both of which are orchestrated by<code>SmartCacheWorkloadLauncher</code>. The MLOS infrastructure consists of the MlosAgent and a communication channel, which are available to both the <code>SmartCacheWorkloadGenerator</code> and the <code>SmartCache</code>.
The <code>SmartCacheWorkloadLauncher</code> launches workloads in <code>SmartCacheWorkloadGenerator</code> in a separate thread, which will actually generate and run the workloads for the smart cache.
The SmartCacheWorkloadLauncher also connects the <code>SmartCacheWorkLloadGenerator</code> to the optimization problem via a <code>MlosAgent</code> that will consume the configurations.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">workload_launcher <span style="color:#f92672">=</span> SmartCacheWorkloadLauncher(logger<span style="color:#f92672">=</span>logger)
mlos_agent <span style="color:#f92672">=</span> workload_launcher<span style="color:#f92672">.</span>mlos_agent
</code></pre></div><p>We set up the agent to consume configurations for the <code>SmartCacheWorkloadGenerator</code>, and we configure the workload to be sequential keys from a range from 0 to 2048.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">mlos_agent<span style="color:#f92672">.</span>set_configuration(
    component_type<span style="color:#f92672">=</span>SmartCacheWorkloadGenerator,
    new_config_values<span style="color:#f92672">=</span>Point(
        workload_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;cyclical_key_from_range&#39;</span>,
        cyclical_key_from_range_config<span style="color:#f92672">=</span>Point(
            min<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>,
            range_width<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>
        )
    )
)
</code></pre></div><h1 id="launching-the-experiment-measurement">Launching the experiment (measurement)</h1>
<p>Now we build the experiment, which collects hit-rate statistics from the <code>SmartCacheWorkloadGenerator</code> via the <code>HitRateMonitor</code>. This architecture reflects the native architecture for the C++ interface in which communication is done via shared memory between MLOS and the worker.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">hit_rate_monitor <span style="color:#f92672">=</span> HitRateMonitor()
smart_cache_experiment <span style="color:#f92672">=</span> MlosExperiment(
    smart_component_types<span style="color:#f92672">=</span>[SmartCache],
    telemetry_aggregators<span style="color:#f92672">=</span>[hit_rate_monitor]
)
mlos_agent<span style="color:#f92672">.</span>start_experiment(smart_cache_experiment)
</code></pre></div><h1 id="performing-the-optimization">Performing the optimization</h1>
<p>Now that we have all the pieces in place, we can iterate our main optimization loop.
Our workload will run in the same process as this notebook, but in a separate thread, which we block on.
In a real example, the workload might run completely independent of our optimization procedure.</p>
<p>We run the optimization for 20 iterations, in each of which we obtain a new configuration from the optimizer (that interfaces the remote optimizer service).
The configuration is passed to <code>SmartCacheWorkloadGenerator</code> via the <code>MlosAgent</code>, after which we start a blocking workload for 0.2 seconds.
Then, the hit-rate (our objective) is read from the <code>HitRateMonitor</code> and the suggested configuration together with the resulting hit-rate are passed to the optimizer.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">num_iterations <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
data <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(num_iterations):
    <span style="color:#75715e"># suggest runs a &#39;cheap&#39; search on the surrogate model to find a good candidate configuration</span>
    new_config_values <span style="color:#f92672">=</span> optimizer<span style="color:#f92672">.</span>suggest()
    <span style="color:#75715e"># set_configuration communicates the proposed configuration to the SmartCache</span>
    mlos_agent<span style="color:#f92672">.</span>set_configuration(component_type<span style="color:#f92672">=</span>SmartCache, new_config_values<span style="color:#f92672">=</span>new_config_values)
    hit_rate_monitor<span style="color:#f92672">.</span>reset()
    <span style="color:#75715e"># start_workload will actually run the worker, here for 0.2 seconds</span>
    workload_launcher<span style="color:#f92672">.</span>start_workload(duration_s<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>, block<span style="color:#f92672">=</span>True)
    <span style="color:#75715e"># obtain hit-rate as quality measure for configuration</span>
    hit_rate <span style="color:#f92672">=</span> hit_rate_monitor<span style="color:#f92672">.</span>get_hit_rate()
    objectives_df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame({<span style="color:#e6db74">&#39;hit_rate&#39;</span>: [hit_rate]})
    <span style="color:#75715e"># pass configuration and observed hit-rate to the optimizer to update the surrogate model</span>
    features_df <span style="color:#f92672">=</span> new_config_values<span style="color:#f92672">.</span>to_dataframe()
    optimizer<span style="color:#f92672">.</span>register(features_df, objectives_df)
    data<span style="color:#f92672">.</span>append((features_df, objectives_df))
    <span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#34;[{i+1}/{num_iterations}] current_config: {new_config_values.to_json()}, hit_rate: {hit_rate:.3f}&#34;</span>)
</code></pre></div><pre><code>[1/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2646}, hit_rate: 0.953
[2/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1980}, hit_rate: 0.000
[3/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 2662}, hit_rate: 0.936
[4/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 312}, hit_rate: 0.000
[5/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 847}, hit_rate: 0.000
[6/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1361}, hit_rate: 0.000
[7/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 507}, hit_rate: 0.230
[8/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1962}, hit_rate: 0.903
[9/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 2689}, hit_rate: 0.939
[10/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1307}, hit_rate: 0.585
[11/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 3481}, hit_rate: 0.952
[12/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 3246}, hit_rate: 0.944
[13/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 2729}, hit_rate: 0.952
[14/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 3755}, hit_rate: 0.946
[15/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1255}, hit_rate: 0.000
[16/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 748}, hit_rate: 0.000
[17/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 790}, hit_rate: 0.000
[18/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 3184}, hit_rate: 0.932
[19/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 606}, hit_rate: 0.258
[20/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1683}, hit_rate: 0.000
[21/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1364}, hit_rate: 0.000
[22/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1311}, hit_rate: 0.000
[23/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1092}, hit_rate: 0.000
[24/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 200}, hit_rate: 0.088
[25/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1238}, hit_rate: 0.569
[26/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2473}, hit_rate: 0.942
[27/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1243}, hit_rate: 0.555
[28/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1068}, hit_rate: 0.492
[29/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1071}, hit_rate: 0.477
[30/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 73}, hit_rate: 0.034
[31/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2291}, hit_rate: 0.951
[32/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1073}, hit_rate: 0.486
[33/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1121}, hit_rate: 0.499
[34/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1412}, hit_rate: 0.000
[35/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1085}, hit_rate: 0.491
[36/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 2556}, hit_rate: 0.950
[37/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2077}, hit_rate: 0.938
[38/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1097}, hit_rate: 0.498
[39/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 3239}, hit_rate: 0.953
[40/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1981}, hit_rate: 0.904
[41/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1950}, hit_rate: 0.898
[42/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1482}, hit_rate: 0.658
[43/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 2004}, hit_rate: 0.922
[44/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 3539}, hit_rate: 0.945
[45/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 492}, hit_rate: 0.000
[46/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1271}, hit_rate: 0.000
[47/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2204}, hit_rate: 0.942
[48/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 305}, hit_rate: 0.143
[49/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 3722}, hit_rate: 0.951
[50/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 3173}, hit_rate: 0.945
[51/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1416}, hit_rate: 0.000
[52/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1072}, hit_rate: 0.475
[53/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 3925}, hit_rate: 0.947
[54/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2140}, hit_rate: 0.926
[55/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 3829}, hit_rate: 0.952
[56/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2179}, hit_rate: 0.945
[57/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 3109}, hit_rate: 0.953
[58/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 102}, hit_rate: 0.045
[59/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 2422}, hit_rate: 0.938
[60/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1983}, hit_rate: 0.914
[61/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2181}, hit_rate: 0.951
[62/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 3622}, hit_rate: 0.944
[63/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2197}, hit_rate: 0.949
[64/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2077}, hit_rate: 0.935
[65/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2077}, hit_rate: 0.951
[66/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1472}, hit_rate: 0.646
[67/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2071}, hit_rate: 0.944
[68/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 536}, hit_rate: 0.000
[69/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 2322}, hit_rate: 0.950
[70/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2064}, hit_rate: 0.933
[71/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2063}, hit_rate: 0.944
[72/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 769}, hit_rate: 0.000
[73/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2067}, hit_rate: 0.944
[74/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2065}, hit_rate: 0.943
[75/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2002}, hit_rate: 0.000
[76/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1903}, hit_rate: 0.000
[77/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 3502}, hit_rate: 0.931
[78/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2032}, hit_rate: 0.000
[79/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2031}, hit_rate: 0.000
[80/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 3755}, hit_rate: 0.951
[81/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2212}, hit_rate: 0.943
[82/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 3357}, hit_rate: 0.950
[83/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1947}, hit_rate: 0.898
[84/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1752}, hit_rate: 0.000
[85/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1985}, hit_rate: 0.904
[86/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2029}, hit_rate: 0.000
[87/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2028}, hit_rate: 0.000
[88/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 3520}, hit_rate: 0.949
[89/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2029}, hit_rate: 0.000
[90/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2030}, hit_rate: 0.000
[91/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 1848}, hit_rate: 0.000
[92/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 3272}, hit_rate: 0.947
[93/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1625}, hit_rate: 0.736
[94/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1615}, hit_rate: 0.730
[95/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2049}, hit_rate: 0.950
[96/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2429}, hit_rate: 0.943
[97/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 2891}, hit_rate: 0.951
[98/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 3690}, hit_rate: 0.941
[99/100] current_config: {&quot;implementation&quot;: &quot;MRU&quot;, &quot;mru_cache_config.cache_size&quot;: 1794}, hit_rate: 0.820
[100/100] current_config: {&quot;implementation&quot;: &quot;LRU&quot;, &quot;lru_cache_config.cache_size&quot;: 2049}, hit_rate: 0.934
</code></pre>
<h1 id="analyzing-results">Analyzing results</h1>
<p>For a cyclical workload with 2048 keys, we assume that a MRU cache with a size of at least 2048 will perform best, and get 100% hits once the cache is filled.
Now lets see the suggestions and results from the current experiment.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># some pandas wrangling</span>

features, targets <span style="color:#f92672">=</span> zip(<span style="color:#f92672">*</span>data)
data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat(features, ignore_index<span style="color:#f92672">=</span>True)
data[<span style="color:#e6db74">&#39;hit_rate&#39;</span>] <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>concat(targets, ignore_index<span style="color:#f92672">=</span>True)
data
</code></pre></div><pre><code>   implementation  lru_cache_config.cache_size  mru_cache_config.cache_size  \
0             LRU                       2646.0                          NaN   
1             LRU                       1980.0                          NaN   
2             MRU                          NaN                       2662.0   
3             LRU                        312.0                          NaN   
4             LRU                        847.0                          NaN   
..            ...                          ...                          ...   
95            LRU                       2429.0                          NaN   
96            MRU                          NaN                       2891.0   
97            LRU                       3690.0                          NaN   
98            MRU                          NaN                       1794.0   
99            LRU                       2049.0                          NaN   

        hit_rate  
0   9.534323e-01  
1   4.019297e-04  
2   9.357813e-01  
3   3.321156e-10  
4   5.120328e-10  
..           ...  
95  9.430359e-01  
96  9.513100e-01  
97  9.409253e-01  
98  8.196099e-01  
99  9.342937e-01  

[100 rows x 4 columns]
</code></pre>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># group by implementation, then plot</span>
lru_data, mru_data <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>groupby(<span style="color:#e6db74">&#39;implementation&#39;</span>)

<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
line_lru <span style="color:#f92672">=</span> lru_data[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>plot(x<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;lru_cache_config.cache_size&#39;</span>, y<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;hit_rate&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;LRU&#39;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>, alpha<span style="color:#f92672">=.</span><span style="color:#ae81ff">6</span>)
mru_data[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>plot(x<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;mru_cache_config.cache_size&#39;</span>, y<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;hit_rate&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;MRU&#39;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>, linestyle<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;none&#39;</span>, alpha<span style="color:#f92672">=.</span><span style="color:#ae81ff">6</span>, ax<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>gca())
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;Cache hitrate&#34;</span>)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;Cache Size&#34;</span>)
plt<span style="color:#f92672">.</span>legend()
</code></pre></div><pre><code>&lt;matplotlib.legend.Legend at 0x13f570169e8&gt;
</code></pre>
<p><img src="/MLOS/notebooks/SmartCacheOptimization_files/SmartCacheOptimization_21_1.png" alt="png" /></p>
<p>We can see that if the cache size is over 2048 keys, it means everything can fit into the cache and the strategy does not matter.
However, for smaller cache sizes, the MRU strategy has an obvious advantage over the LRU strategy.</p>
<h1 id="going-further">Going Further</h1>
<ol>
<li>
<p>Log how the optimum evolves over time. How many iterations are needed?</p>
</li>
<li>
<p>Can you adjust options in the Optimizer to improve convergence (see the BayesianOptimization notebook for suggestions).</p>
</li>
<li>
<p>Choose a different workload in the <code>SmartCacheWorkloadGenerator</code>. How do the workloads change the optimum strategy?</p>
</li>
</ol>
<h1 id="clean-up">Clean up</h1>
<p>We need to stop all processes &amp; separate threads after running the experiments:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Clean up</span>
<span style="color:#75715e">#</span>
mlos_agent<span style="color:#f92672">.</span>stop_experiment(smart_cache_experiment)
mlos_agent<span style="color:#f92672">.</span>stop_all()

<span style="color:#75715e"># Stop the optimizer service</span>
<span style="color:#f92672">import</span> signal
optimizer_microservice<span style="color:#f92672">.</span>send_signal(signal<span style="color:#f92672">.</span>SIGTERM)
</code></pre></div><h1 id="download-smartcacheoptimizationipynb-notebookhttpsrawgithubusercontentcommicrosoftmlosmainsourcemlosnotebookssmartcacheoptimizationipynb-1"><a href="https://raw.githubusercontent.com/microsoft/MLOS/main/source/Mlos.Notebooks/SmartCacheOptimization.ipynb">Download SmartCacheOptimization.ipynb notebook</a></h1>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">





</div>

 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      
  <nav id="TableOfContents"></nav>

 
    </aside>
    
  </main>

  
</body>

</html>












