<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MLOS</title>
    <link>https://microsoft.github.io/MLOS/</link>
    <description>Recent content on MLOS</description>
    <generator>Hugo -- gohugo.io</generator>
    
	<atom:link href="https://microsoft.github.io/MLOS/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/CODE_OF_CONDUCT/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/CODE_OF_CONDUCT/</guid>
      <description>Microsoft Open Source Code of Conduct This project has adopted the Microsoft Open Source Code of Conduct.
Resources:
 Microsoft Open Source Code of Conduct Microsoft Code of Conduct FAQ Contact opencode@microsoft.com with questions or concerns  </description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/CONTRIBUTING/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/CONTRIBUTING/</guid>
      <description>Contributing to MLOS This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.
When you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment).</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/01-Prerequisites/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/01-Prerequisites/</guid>
      <description>Prerequisites for building and using MLOS These are one-time setup instructions that should be executed prior to following the build instructions in 02-Build.md
Contents  Prerequisites for building and using MLOS  Contents Cloning the repository Python quickstart Linux  Linux Distribution Requirements Option 1: Linux Docker Build Env  Install Docker Build the Docker Image   Option 2: Manual Build Tools Install Install Python on Linux  Option 1: Docker Python Install Option 2: Using Conda     Windows  Step 1: Install Python Step 2: Install Docker on Windows Step 3: Install Windows Build Tools      MLOS currently supports 64-bit Intel/AMD platforms, though ARM64 support is under development.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/02-Build/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/02-Build/</guid>
      <description>Build Instructions for MLOS Prerequisites See 01-Prerequisites.md for initial build tools setup instructions.
There are different instructions according to the environment setup you chose.
Contents  Build Instructions for MLOS  Prerequisites Contents Docker  Create a new container instance Other useful docker commands Start an existing container instance Get a new shell in a running container instance   Linux  CLI: make VSCode   Windows  CLI: msbuild Building with Visual Studio      Docker If you chose to use the Docker build environment and have already built or pulled a container image using the instructions in 01-Prerequisites.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/03-ExampleUsage/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/03-ExampleUsage/</guid>
      <description>Examples of using MLOS to optimize a system TODO</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/04-Test/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/04-Test/</guid>
      <description>Test Instructions for MLOS Contents  Test Instructions for MLOS  Contents Linux Tests  Run C# Tests on Linux Run C++ Tests on Linux Run Python Tests on Linux   Windows  Run C#/C++ Tests on Windows Run Python Tests on Windows      Linux Tests To build and test all of the MLOS code at or below the current folder, regardless of language, run:</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/05-Debug/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/05-Debug/</guid>
      <description>TODO</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/CodingStandard/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/CodingStandard/</guid>
      <description>MLOS Coding Standards MLOS uses and supports multiple languages. Here we document the coding styles and standards we attempt to adhere to and the tools we use to achieve that.
C++ For C++ we mostly try to follow the Google C++ style guidelines, with a few modifications.
Currently we rely on uncrustify to help enforce these rules (plus a little bit of human review).
See build/uncrustify/README.md for additional information.
Though we attempt to make it somewhat readable, we exclude code generated by MLOS from these strict style checks.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/Glossary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/Glossary/</guid>
      <description>MLOS Terms Glossary TODO</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/MlosArchitecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/MlosArchitecture/</guid>
      <description>MLOS Architecture This document provides a brief overview of the MLOS architecture for supporting Machine Learning Optimized Systems.
 MLOS Architecture  High Level Description  Principles Workflows   Architecture Diagram  Main components Shared Memory Regions Target Process  Mlos.Core Shared Channel   Mlos.Agent  Mlos.NetCore Settings registry assemblies Grpc Server Experiment management     Implementation details    High Level Description At a high level, MLOS provides infrastructure to support instance-specific tuning systems software (e.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/documentation/RepoOrganization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/documentation/RepoOrganization/</guid>
      <description>Repo Organization Some notes on the directory layout organization in this repo.
 There are build files (e.g. dirs.proj for msbuild or dotnet build, or Makefiles for make) in most directories to allow easy recursive building of that subtree you happen to be in.  Note: we provide Makefile wrappers in most directories to simply help invoke cmake and the Makefiles it generates
  build/ contains configuration related to building MLOS components  For instance, .</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/notebooks/BayesianOptimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/notebooks/BayesianOptimization/</guid>
      <description>Download BayesianOptimization.ipynb notebook import matplotlib.pyplot as plt import numpy as np import pandas as pd Bayesian Optimization This notebook demonstrates the basic principles of Bayesian Optimization (BO) and how to use MLOS to perform BO.
Motivation In software performance engineering, the impact different (input) parameters (e.g. buffer size, worker thread count, etc.) can have on the (output) performance of a system for a given workload (input) can be modeled as a multidimensional function - one which we don&amp;rsquo;t know the equation for apriori, but are instead trying to learn through careful sampling of the input space and experimentation (test/benchmark runs) to gather output points.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/notebooks/SmartCacheOptimization/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/notebooks/SmartCacheOptimization/</guid>
      <description>Download SmartCacheOptimization.ipynb notebook Optimizing Smart Cache with Bayesian Optimization The goal of this notebook is to optimize SmartCache using Bayesian Optimization approach.
We&amp;rsquo;re using a sequential model-based optimization approach, that consists of the following loop:
 Get suggested config from optimizer, Apply suggested config to SmartCache, Execute a fixed workload, Collect the metrics from SmartCache, Register an observation with the optimizer.  # import the required classes and tools import grpc import pandas as pd import logging from mlos.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/SECURITY/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/SECURITY/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Core/doc/SharedChannel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Core/doc/SharedChannel/</guid>
      <description>MLOS Shared Channel This document describes the implementation details of the mechanism (a shared memory communication channel) used for a target system to communicate with an external agent for tuning it.
For additional context, please see the MlosArchitecture.md documentation.
Contents  MLOS Shared Channel  Shared Channel  Principles: Circular buffer algorithm  Writer Reader Writer continued Reader continued Cyclic buffer handling   Scaling out readers   Shared channel implementation  Diagram Policies Notes Links      Shared Channel A shared channel is a one-directional communication channel based on a single shared memory block (i.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Core/doc/SharedMemoryManagement/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Core/doc/SharedMemoryManagement/</guid>
      <description>MemoryRegion SharedMemoryView @import &amp;ldquo;./images/SharedMemoryManagement.svg&amp;rdquo; {width=&amp;quot;1400px&amp;rdquo; height=&amp;quot;1000px&amp;rdquo; title=&amp;quot;Shared memory regions&amp;rdquo;}</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.NetCore/Doc/SharedChannelScaleout/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.NetCore/Doc/SharedChannelScaleout/</guid>
      <description>Shared Channel Scaleout Document contains results of the improvement in the shared communication channel. We specifically address lack of the scalability issue. The channel does not scale well with increasing number of readers and writers operating on the same channel instance.
Benchmark The benchmark is implemented in Mlos.NetCore.Benchmark project
Benchmark:
 $(MLOSROOT)\MLOS\out\obj\Source\Mlos.NetCore.Benchmark\obj\amd64\Mlos.NetCore.Benchmark.exe -i &amp;ndash;filter SharedChannelReaderScaleOutBenchmarks
 Results Hyper-threading is disabled.
 Intel E5-2670 v3 @ 2.30 GHz:    ReaderCount Mean[B] Mean[I] Error[B] Error[I] StdDev[B] StdDev[I] Allocated     1 830.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/BayesianOptimizerArchitecture/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/BayesianOptimizerArchitecture/</guid>
      <description>Bayesian Optimizer Architecture Components  Surrogate Models Utility Functions Numeric Optimizers Experiment Designer Bayesian Optimizer  Other Classes  Hypergrids Optimization Problems  Hypergrids Hypergrids are used to describe multidimensional spaces comprized of Continuous, Discrete, Ordinal, or Categorical dimensions.
All optimizers we have reviewed to date (Hypermapper, SMAC, bayesopt, and scikit-learn) optimizers implement their own notion of a search space.
Hypergrids are meant to provide a superset of their functionalities. They further allow us to express hierarchical search spaces, where some parameters only become meaningful if some other parameter (e.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/BayesianOptimizerV1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/BayesianOptimizerV1/</guid>
      <description>Bayesian Optimizer V1 The goal of this document is to describe the architecture and inner workings of a Bayesian Optimizer.
Components The following components will be necessary:
 Experiment Observation Storage (Table in SQL Server) Bayesian Optimizer Surrogate Models  Architecture TODO: describe how we will use the technologies:
 Docker (K8S?) SQL Server Python ML.Net SQL DB RPC vs. gRPC  Obviously an experiment and the corresponding bayesian optimizer have to be compatible so that the observations generated by the experiment are within the optimizers observation space.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/OptimizerMonitoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/OptimizerMonitoring/</guid>
      <description>Optimizer Monitoring Motivation The goal of this document is to outline the process of monitoring the optimizers, enumerate the metrics we wish to collect and the tools required to do so.
What we wish to monitor For each optimizer we should be able to:
 View it&amp;rsquo;s current configuration (done). View all the data that it was trained on. View the state of the surrogate models:  Have they been fitted?</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/SimpleImportGraph/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/Docs/SimpleImportGraph/</guid>
      <description>digraph dfd2 {node[shape=record]&amp;quot;BayesianOptimizer.py&amp;quot; -&amp;gt; &amp;quot;OptimizationProblem.py&amp;quot;&amp;quot;HomogeneousRandomForestRegressionModel.py&amp;quot; -&amp;gt; &amp;quot;DecisionTreeRegressionModel.py&amp;quot;&amp;quot;BayesianOptimizer.py&amp;quot; -&amp;gt; &amp;quot;OptimizerInterface.py&amp;quot;&amp;quot;ExperimentDesigner.py&amp;quot; -&amp;gt; &amp;quot;RegressionModel.py&amp;quot;&amp;quot;OptimizerInterface.py&amp;quot; -&amp;gt; &amp;quot;OptimizationProblem.py&amp;quot;&amp;quot;BayesianOptimizer.py&amp;quot; -&amp;gt; &amp;quot;ExperimentDesigner.py&amp;quot;&amp;quot;HomogeneousRandomForestRegressionModel.py&amp;quot; -&amp;gt; &amp;quot;Prediction.py&amp;quot;&amp;quot;DecisionTreeRegressionModel.py&amp;quot; -&amp;gt; &amp;quot;RegressionModel.py&amp;quot;&amp;quot;ExperimentDesigner.py&amp;quot; -&amp;gt; &amp;quot;ConfidenceBoundUtilityFunction.py&amp;quot;&amp;quot;RandomSearchOptimizer.py&amp;quot; -&amp;gt; &amp;quot;OptimizationProblem.py&amp;quot;&amp;quot;HomogeneousRandomForestRegressionModel.py&amp;quot; -&amp;gt; &amp;quot;RegressionModel.py&amp;quot;&amp;quot;DecisionTreeRegressionModel.py&amp;quot; -&amp;gt; &amp;quot;Prediction.py&amp;quot;&amp;quot;ExperimentDesigner.py&amp;quot; -&amp;gt; &amp;quot;RandomSearchOptimizer.py&amp;quot;&amp;quot;ExperimentDesigner.py&amp;quot; -&amp;gt; &amp;quot;OptimizationProblem.py&amp;quot;&amp;quot;BayesianOptimizer.py&amp;quot; -&amp;gt; &amp;quot;HomogeneousRandomForestRegressionModel.py&amp;quot;}</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/mlos/Examples/SmartCache/OverviewOfCachingStrategies/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/mlos/Examples/SmartCache/OverviewOfCachingStrategies/</guid>
      <description>Overview of Caching Strategies and Algorithms Goal The objective of this document is to describe various approaches to caching. The intent is to then implement a number of paramiterized caching algorithms and allow MLOS to select between them on a per-workload basis.
Links A loose list of sources consulted in this survey:
 https://medium.com/datadriveninvestor/all-things-caching-use-cases-benefits-strategies-choosing-a-caching-technology-exploring-fa6c1f2e93aa https://en.wikipedia.org/wiki/Cache_replacement_policies  Potential Objectives  average retrieval time/cost/latency (or other statistics: percentiles, CI&amp;rsquo;s etc) hit ratio and miss ratio cache hit latency metrics (in case of a hit) data staleness metrics (distribution of time since last usage among all cache entries)  Plausible Implementations/Approaches  FIFO - a queue and a hash-map would work.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Python/mlos/Spaces/HypergridAdapters/AboutHypergridAdapters/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Python/mlos/Spaces/HypergridAdapters/AboutHypergridAdapters/</guid>
      <description>Hypergrid Adapters Motivation Categorical to numeric projections The goal of adapters is to make a broader class of hypergrids compatible with any surrogate model. The chief problem in absence of adapters is that some models (RERF, DecisionTreeRegressionModel) can only operate on numeric datatypes, but the configuration for many components includes strings, and booleans as well. A suitable adapter will map such categorical dimensions into numeric ones and allow transparent projection of observations and suggestions between the components and the regression models.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.SettingsSystem.CodeGen/Doc/CodeGen/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.SettingsSystem.CodeGen/Doc/CodeGen/</guid>
      <description>#CodeGen
Intro Document describes internal CodeGen implementation.
MLOS goals Provide an ability to expose internal application settings, telemetry, and events to an external agent. Codegen responsibility is to generate a set of classes and helper methods to exchange the messages and share (read and update) config structures residing in the shared memory.
Why custom CodeGen? Not Protocol Buffers, FlatBuffers, XEvents etc&amp;hellip;
The answer is performance and full control. Emitting the telemetry will occur on the hot path (not always).</description>
    </item>
    
    <item>
      <title></title>
      <link>https://microsoft.github.io/MLOS/source/Mlos.Streaming/Doc/AggregateStreaming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://microsoft.github.io/MLOS/source/Mlos.Streaming/Doc/AggregateStreaming/</guid>
      <description>Aggregates on the telemetry streams Intro Processing models     Single return value Multiple return values     Pull/Synchronous/Interactive T IEnumerable&amp;lt;T&amp;gt;   Push/Asynchronous/Reactive Task&amp;lt;T&amp;gt; IObservable&amp;lt;T&amp;gt;    MLOS Telemetry channel Processing events   why not async:
 introduces dedicated processing tasks additional latency introduced by AsyncQueue    why we need a push model
  Linq operators on observable streams Merging streams operator Collecting the results  in progress  Performance improvements  in progress  </description>
    </item>
    
  </channel>
</rss>