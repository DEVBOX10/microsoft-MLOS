{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "The goal of this notebook is to optimize SmartCache. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Lots to do here.\n",
    "\n",
    "For once - this is probably not the workflow we want. We likely want to:\n",
    "1. <s>Start the Optimizer in a separate process so that we can monitor its progress. Maybe we should have a separate notebook just for Optimizer Monitoring.</s>\n",
    "1. Start the Mlos.Agent + Workload in a separate process. Perhaps communicate with them over gRPC. This would bring us closer to the C# scenario but is not a top priority.\n",
    "1. We need to be able to see how well the cache is doing: what are its stats.\n",
    "1. We want to modify the workload so that one configuration dominates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "from subprocess import Popen, CREATE_NEW_CONSOLE\n",
    "import sys\n",
    "from threading import Thread\n",
    "\n",
    "import grpc\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlos.Grpc.BayesianOptimizerFactory import BayesianOptimizerFactory\n",
    "from mlos.Grpc.BayesianOptimizerProxy import BayesianOptimizerProxy\n",
    "from mlos.Grpc.OptimizerMonitor import OptimizerMonitor\n",
    "\n",
    "from mlos.Logger import create_logger\n",
    "\n",
    "from mlos.Examples.SmartCache import SmartCacheWorkloadGenerator, SmartCache\n",
    "from mlos.Examples.SmartCache.TelemetryAggregators.WorkingSetSizeEstimator import WorkingSetSizeEstimator\n",
    "\n",
    "from mlos.Grpc.OptimizerMicroserviceServer import OptimizerMicroserviceServer\n",
    "\n",
    "from mlos.Mlos.Infrastructure import CommunicationChannel, SharedConfig\n",
    "from mlos.Mlos.SDK import mlos_globals, MlosGlobalContext, MlosExperiment, MlosAgent\n",
    "from mlos.Mlos.SDK.CommonAggregators.Timer import Timer\n",
    "\n",
    "from mlos.Optimizers.BayesianOptimizer import BayesianOptimizerConfig\n",
    "from mlos.Optimizers.OptimizationProblem import OptimizationProblem, Objective\n",
    "from mlos.Spaces import Point, SimpleHypergrid, ContinuousDimension\n",
    "\n",
    "import mlos.global_values as global_values\n",
    "\n",
    "grpc_port = 50051\n",
    "mlos_python_path = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"Mlos.Python\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's stand up the Optimizer Service\n",
    "#\n",
    "python_path = sys.executable\n",
    "optimizer_microservice_launcher_path = os.path.abspath(os.path.join(mlos_python_path, \"mlos\", \"start_optimizer_microservice.py\"))\n",
    "\n",
    "command = f\"{python_path} {optimizer_microservice_launcher_path} launch --port {grpc_port}\"\n",
    "print(command)\n",
    "server_process = Popen(command, creationflags=CREATE_NEW_CONSOLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = create_logger('Optimizing Smart Cache')\n",
    "optimizer_service_grpc_channel = grpc.insecure_channel(f'localhost:{grpc_port}')\n",
    "bayesian_optimizer_factory = BayesianOptimizerFactory(grpc_channel=optimizer_service_grpc_channel, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization Problem\n",
    "#\n",
    "optimization_problem = OptimizationProblem(\n",
    "    parameter_space=SmartCache.parameter_search_space,\n",
    "    objective_space=SimpleHypergrid(name=\"objectives\", dimensions=[ContinuousDimension(name=\"miss_rate\", min=0, max=1)]),\n",
    "    context_space=None, # TODO add the working set size estimators.\n",
    "    objectives=[Objective(name=\"miss_rate\", minimize=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = bayesian_optimizer_factory.create_remote_optimizer(\n",
    "    optimization_problem=optimization_problem,\n",
    "    optimizer_config=BayesianOptimizerConfig.DEFAULT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now onto the mlos global context - this is where the smart component can send its telemetry and check for new config.\n",
    "# Note that the communication channel and shared config are - well - shared between mlos agent and the smart component.\n",
    "#\n",
    "global_values.declare_singletons()\n",
    "communication_channel = CommunicationChannel()\n",
    "shared_config = SharedConfig()\n",
    "mlos_globals.init_mlos_global_context()\n",
    "mlos_globals.mlos_global_context = MlosGlobalContext(communication_channel=communication_channel, shared_config=shared_config)\n",
    "# Now let's create the MlosAgent. Note that this architecture mirrors our native architecture, except it's much less performant.\n",
    "#\n",
    "\n",
    "mlos_agent = MlosAgent(\n",
    "    logger=logger,\n",
    "    communication_channel=communication_channel,\n",
    "    shared_config=shared_config,\n",
    "    mlos_service_endpoint=None, # TODO: remove this completely from everywhere.\n",
    "    bayesian_optimizer_grpc_channel=optimizer_service_grpc_channel\n",
    ")\n",
    "\n",
    "mlos_agent_thread = Thread(target=mlos_agent.run)\n",
    "mlos_agent_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's register SmartCache as a legal smart component.\n",
    "#\n",
    "mlos_agent.add_allowed_component_type(SmartCache)\n",
    "mlos_agent.add_allowed_component_type(SmartCacheWorkloadGenerator) # TODO: for the very first exercise we should have a fixed workload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_cache_workload = SmartCacheWorkloadGenerator(logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the telemetry aggregators used in the experiments.\n",
    "#\n",
    "working_set_size_estimator = WorkingSetSizeEstimator()\n",
    "\n",
    "\n",
    "# TODO: this should be yet another telemetry aggregator, not a standalone function.\n",
    "#\n",
    "def set_new_cache_configuration(elapsed_time_ms):\n",
    "    # TODO: remove the globals.\n",
    "    global logger\n",
    "    global working_set_size_estimator\n",
    "    global optimizer\n",
    "    global mlos_agent\n",
    "    \n",
    "    # Just to make sure the optimizer monitor and the tomograph are working: let's register some dummy results.\n",
    "    # TODO: aggregate telemetry to determine the true miss rate for this cache for a given config.\n",
    "    #\n",
    "    existing_config = mlos_agent.get_configuration(component_type=SmartCache)\n",
    "    if existing_config is not None:\n",
    "        features_df = existing_config.to_pandas()\n",
    "\n",
    "        if existing_config.implementation == \"MRU\":\n",
    "            miss_rate = -1000 + existing_config.mru_cache_config.cache_size\n",
    "        else:\n",
    "            miss_rate = 1000 + existing_config.lru_cache_config.cache_size\n",
    "\n",
    "        objectives_df = pd.DataFrame({'miss_rate': [miss_rate]})\n",
    "        optimizer.register(features_df, objectives_df)    \n",
    "    \n",
    "    # TODO: have the working set size estimate be part of the context passed to the optimizer.\n",
    "    #\n",
    "    current_estimate = working_set_size_estimator.estimate_working_set_size()\n",
    "    logger.info(f\"Estimated working set size: {current_estimate.chapman_estimator}\")\n",
    "    \n",
    "    new_config_values = optimizer.suggest()\n",
    "    mlos_agent.set_configuration(\n",
    "    component_type=SmartCache,\n",
    "        new_config_values=new_config_values\n",
    "    )\n",
    "    \n",
    "\n",
    "# TODO: we need aggregators to spit out hit rate, as well as other Cache Info (cache entry staleness, cache saturation, etc.)\n",
    "\n",
    "cache_config_timer = Timer(\n",
    "    timeout_ms=200,\n",
    "    observer_callback=set_new_cache_configuration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now finally we build the experiment.\n",
    "#\n",
    "smart_cache_experiment = MlosExperiment(\n",
    "    smart_component_types=[SmartCache],\n",
    "    telemetry_aggregators=[cache_config_timer, working_set_size_estimator]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We have the experiment, let's start it.\n",
    "#\n",
    "smart_cache_workload_thread = Thread(target=smart_cache_workload.run, args=(150,))\n",
    "smart_cache_workload_thread.start()\n",
    "mlos_agent.start_experiment(smart_cache_experiment)\n",
    "\n",
    "smart_cache_workload_thread.join()\n",
    "mlos_agent.stop_experiment(smart_cache_experiment)\n",
    "mlos_globals.mlos_global_context.stop_clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "#\n",
    "mlos_globals.mlos_global_context.stop_clock()\n",
    "mlos_agent.stop_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill the server.\n",
    "#\n",
    "server_process.kill()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
